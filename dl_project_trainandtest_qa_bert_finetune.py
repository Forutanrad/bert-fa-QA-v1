# -*- coding: utf-8 -*-
"""DL-Project-TrainAndTest-QA-bert-fineTune.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WsJDHfo7-e7iJDqfmAuwxQzkaVomhNPp

**Deep Learning Project**
> Javad ForutanRad


---

#**1- Learn model**

## **1-1 Install Library Needed**
"""

!pip install -q transformers
!pip install -q datasets

# pip install hazm

"""## **1-2 Call Library Needed**"""

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
from transformers import TrainingArguments, Trainer

"""## **1-3 Set Parameter Value**"""

model_checkpoint = "HooshvareLab/bert-fa-base-uncased" #   ParsBERT MODEL v2 
max_length = 512 # The maximum length of a feature (question and context)
doc_stride = 256 # The authorized overlap between two part of the context when splitting it is needed.
batch_size = 8 
lr = 3e-5
epoch = 2

"""## **1-4 Load dataset**"""

datasets = load_dataset("SajjadAyoubi/persian_qa")
datasets['train'][0]

"""## **1-5 model Tokenizer**"""

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
print(tokenizer)

tokenized_examples = tokenizer(
    datasets['train'][0]["question"],
    datasets['train'][0]["context"],
    truncation="only_second",
    max_length=max_length,
    stride=doc_stride,
    return_overflowing_tokens=True,
    return_offsets_mapping=True,)

sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
offset_mapping = tokenized_examples.pop("offset_mapping")

print(sample_mapping)
print(len(offset_mapping))
print(tokenized_examples["input_ids"][0])
print(tokenizer.cls_token_id)

"""## **1-6  Prepare Train Features Function**"""

def prepare_train_features(examples):
    tokenized_examples = tokenizer(
        examples["question"],
        examples["context"],
        truncation="only_second",
        max_length=max_length,
        stride=doc_stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,)
    
    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
    offset_mapping = tokenized_examples.pop("offset_mapping")
    tokenized_examples["start_positions"] = []
    tokenized_examples["end_positions"] = []
    for i, offsets in enumerate(offset_mapping):
        # We will label impossible answers with the index of the CLS token.
        input_ids = tokenized_examples["input_ids"][i]
        cls_index = input_ids.index(tokenizer.cls_token_id)
        # Grab the sequence corresponding to that example (to know what is the context and what is the question).
        sequence_ids = tokenized_examples.sequence_ids(i)
        # One example can give several spans, this is the index of the example containing this span of text.
        sample_index = sample_mapping[i]
        answers = examples["answers"][sample_index]
        # If no answers are given, set the cls_index as answer.
        if len(answers["answer_start"]) == 0:
            tokenized_examples["start_positions"].append(cls_index)
            tokenized_examples["end_positions"].append(cls_index)
        else:
            # Start/end character index of the answer in the text.
            start_char = answers["answer_start"][0]
            end_char = start_char + len(answers["text"][0])
            # Start token index of the current span in the text.
            token_start_index = 0
            while sequence_ids[token_start_index] != 1:
                token_start_index += 1
            # End token index of the current span in the text.
            token_end_index = len(input_ids) - 1
            while sequence_ids[token_end_index] != 1:
                token_end_index -= 1
            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).
            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):
                tokenized_examples["start_positions"].append(cls_index)
                tokenized_examples["end_positions"].append(cls_index)
            else:
                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.
                # Note: we could go after the last offset if the answer is the last word (edge case).
                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
                    token_start_index += 1
                tokenized_examples["start_positions"].append(token_start_index - 1)
                while offsets[token_end_index][1] >= end_char:
                    token_end_index -= 1
                tokenized_examples["end_positions"].append(token_end_index + 1)

    return tokenized_examples

# the datasets library does cashing itself, batched is multitreading for fast-tokenizer
tokenized_ds = datasets.map(prepare_train_features, batched=True, remove_columns=datasets["train"].column_names)

"""## **1-7 Train Model**"""

model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

args = TrainingArguments(
    evaluation_strategy = "epoch",
    learning_rate=lr,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=epoch,
    weight_decay=0.001
 )

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_ds['train'],
    eval_dataset=tokenized_ds['validation'],
    tokenizer=tokenizer)

# start training
trainer.train()

"""# **2- Model Execute**

## 2-1 Preprocess Script
"""

!pip install -q transformers
!pip install -q sentencepiece
!pip install hazm

from tqdm import tqdm
from IPython.display import clear_output
import torch
from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model_name = 'ForutanRad/bert-fa-QA-v1'
model = AutoModelForQuestionAnswering.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
clear_output()

class AnswerPredictor:
  def __init__(self, model, tokenizer, device='cuda', n_best=10, max_length=512, stride=256, no_answer=False):
      """Initializes PyTorch Question Answering Prediction
      It's best to leave use the default values.
      Args:
          model: Fine-tuned torch model
          tokenizer: Transformers tokenizer
          device (torch.device): Running device
          n_best (int): Number of best possible answers
          max_length (int): Tokenizer max length
          stride (int): Tokenizer stride
          no_answer (bool): If True, model can return "no answer"
      """
      device = 'cuda' if torch.cuda.is_available() else 'cpu'

      self.model = model.eval().to(device)
      self.tokenizer = tokenizer
      self.device = device
      self.max_length = max_length
      self.stride = stride
      self.no_answer = no_answer
      self.n_best = n_best


  def model_pred(self, questions, contexts, batch_size=1):
      n = len(contexts)
      if n%batch_size!=0:
          raise Exception("batch_size must be divisible by sample length")

      tokens = self.tokenizer(questions, contexts, add_special_tokens=True, 
                              return_token_type_ids=True, return_tensors="pt", padding=True, 
                              return_offsets_mapping=True, truncation="only_second", 
                              max_length=self.max_length, stride=self.stride)

      start_logits, end_logits = [], []
      for i in tqdm(range(0, n-batch_size+1, batch_size)):
          with torch.no_grad():
              out = self.model(tokens['input_ids'][i:i+batch_size].to(self.device), 
                          tokens['attention_mask'][i:i+batch_size].to(self.device), 
                          tokens['token_type_ids'][i:i+batch_size].to(self.device))

              start_logits.append(out.start_logits)
              end_logits.append(out.end_logits)

      return tokens, torch.stack(start_logits).view(n, -1), torch.stack(end_logits).view(n, -1)


  def __call__(self, questions, contexts, batch_size=1, answer_max_len=100):
      """Creates model prediction
      
      Args: 
          questions (list): Question strings
          contexts (list): Contexts strings
          batch_size (int): Batch size
          answer_max_len (int): Sets the longests possible length for any answer
        
      Returns:
          dict: The best prediction of the model
              (e.g {0: {"text": str, "score": int}})
      """
      tokens, starts, ends = self.model_pred(questions, contexts, batch_size=batch_size)
      start_indexes = starts.argsort(dim=-1, descending=True)[:, :self.n_best]
      end_indexes = ends.argsort(dim=-1, descending=True)[:, :self.n_best]

      preds = {}
      for i, (c, q) in enumerate(zip(contexts, questions)):  
          min_null_score = starts[i][0] + ends[i][0] # 0 is CLS Token
          start_context = tokens['input_ids'][i].tolist().index(self.tokenizer.sep_token_id)
          
          offset = tokens['offset_mapping'][i]
          valid_answers = []
          for start_index in start_indexes[i]:
              # Don't consider answers that are in questions
              if start_index<start_context:
                  continue
              for end_index in end_indexes[i]:
                  # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond
                  # to part of the input_ids that are not in the context.
                  if (start_index >= len(offset) or end_index >= len(offset)
                      or offset[start_index] is None or offset[end_index] is None):
                      continue
                  # Don't consider answers with a length that is either < 0 or > max_answer_length.
                  if end_index < start_index or (end_index-start_index+1) > answer_max_len:
                      continue

                  start_char = offset[start_index][0]
                  end_char = offset[end_index][1]
                  valid_answers.append({"score": (starts[i][start_index] + ends[i][end_index]).item(),
                                        "text": c[start_char: end_char]})
                  
          if len(valid_answers) > 0:
              best_answer = sorted(valid_answers, key=lambda x: x["score"], reverse=True)[0]
          else:
              best_answer = {"score": min_null_score, "text": ""}

          if self.no_answer:
              preds[i] = best_answer if best_answer["score"] >= min_null_score else {"text": "", "score": min_null_score}
          else:
              preds[i] = best_answer
      return preds

predictor = AnswerPredictor(model, tokenizer, device='cuda', n_best=10, no_answer=True)

"""## **2-2 Select Normalizer**"""

from hazm import *
normalizer = Normalizer()

"""## **2-3 Run Model**

Paste your Context and ask a Question
"""

#  Paste your Context and ask a Question
context = "\u0634\u0631\u06A9\u062A \u0641\u0648\u0644\u0627\u062F \u0645\u0628\u0627\u0631\u06A9\u06C0 \u0627\u0635\u0641\u0647\u0627\u0646\u060C \u0628\u0632\u0631\u06AF\u200C\u062A\u0631\u06CC\u0646 \u0648\u0627\u062D\u062F \u0635\u0646\u0639\u062A\u06CC \u062E\u0635\u0648\u0635\u06CC \u062F\u0631 \u0627\u06CC\u0631\u0627\u0646 \u0648 \u0628\u0632\u0631\u06AF\u200C\u062A\u0631\u06CC\u0646 \u0645\u062C\u062A\u0645\u0639 \u062A\u0648\u0644\u06CC\u062F \u0641\u0648\u0644\u0627\u062F \u062F\u0631 \u06A9\u0634\u0648\u0631 \u0627\u06CC\u0631\u0627\u0646 \u0627\u0633\u062A\u060C \u06A9\u0647 \u062F\u0631 \u0634\u0631\u0642 \u0634\u0647\u0631 \u0645\u0628\u0627\u0631\u06A9\u0647 \u0642\u0631\u0627\u0631 \u062F\u0627\u0631\u062F. \u0641\u0648\u0644\u0627\u062F \u0645\u0628\u0627\u0631\u06A9\u0647 \u0647\u0645\u200C\u0627\u06A9\u0646\u0648\u0646 \u0645\u062D\u0631\u06A9 \u0628\u0633\u06CC\u0627\u0631\u06CC \u0627\u0632 \u0635\u0646\u0627\u06CC\u0639 \u0628\u0627\u0644\u0627\u062F\u0633\u062A\u06CC \u0648 \u067E\u0627\u06CC\u06CC\u0646\u200C\u062F\u0633\u062A\u06CC \u0627\u0633\u062A. \u0641\u0648\u0644\u0627\u062F \u0645\u0628\u0627\u0631\u06A9\u0647 \u062F\u0631 \u06F1\u06F1 \u062F\u0648\u0631\u0647 \u062C\u0627\u06CC\u0632\u06C0 \u0645\u0644\u06CC \u062A\u0639\u0627\u0644\u06CC \u0633\u0627\u0632\u0645\u0627\u0646\u06CC \u0648 \u06F6 \u062F\u0648\u0631\u0647 \u062C\u0627\u06CC\u0632\u06C0 \u0634\u0631\u06A9\u062A \u062F\u0627\u0646\u0634\u06CC \u062F\u0631 \u06A9\u0634\u0648\u0631 \u0631\u062A\u0628\u06C0 \u0646\u062E\u0633\u062A \u0631\u0627 \u0628\u062F\u0633\u062A \u0622\u0648\u0631\u062F\u0647\u200C\u0627\u0633\u062A \u0648 \u0647\u0645\u0686\u0646\u06CC\u0646 \u0627\u06CC\u0646 \u0634\u0631\u06A9\u062A \u062F\u0631 \u0633\u0627\u0644 \u06F1\u06F3\u06F9\u06F1 \u0628\u0631\u0627\u06CC \u0646\u062E\u0633\u062A\u06CC\u0646\u200C\u0628\u0627\u0631 \u0628\u0647 \u0639\u0646\u0648\u0627\u0646 \u062A\u0646\u0647\u0627 \u0634\u0631\u06A9\u062A \u0627\u06CC\u0631\u0627\u0646\u06CC \u0628\u0627 \u06A9\u0633\u0628 \u0627\u0645\u062A\u06CC\u0627\u0632 \u06F6\u06F5\u06F4 \u062A\u0646\u062F\u06CC\u0633 \u0632\u0631\u06CC\u0646 \u062C\u0627\u06CC\u0632\u06C0 \u0645\u0644\u06CC \u062A\u0639\u0627\u0644\u06CC \u0633\u0627\u0632\u0645\u0627\u0646\u06CC \u0631\u0627 \u0627\u0632 \u0622\u0646 \u062E\u0648\u062F \u06A9\u0646\u062F. \u0634\u0631\u06A9\u062A \u0641\u0648\u0644\u0627\u062F \u0645\u0628\u0627\u0631\u06A9\u06C0 \u0627\u0635\u0641\u0647\u0627\u0646 \u062F\u0631 \u06F2\u06F3 \u062F\u06CC \u0645\u0627\u0647 \u06F1\u06F3\u06F7\u06F1 \u0627\u062D\u062F\u0627\u062B \u0634\u062F \u0648 \u0627\u06A9\u0646\u0648\u0646 \u0628\u0632\u0631\u06AF\u200C\u062A\u0631\u06CC\u0646 \u0648\u0627\u062D\u062F\u0647\u0627\u06CC \u0635\u0646\u0639\u062A\u06CC \u0648 \u0628\u0632\u0631\u06AF\u062A\u0631\u06CC\u0646 \u0645\u062C\u062A\u0645\u0639 \u062A\u0648\u0644\u06CC\u062F \u0641\u0648\u0644\u0627\u062F \u062F\u0631 \u0627\u06CC\u0631\u0627\u0646 \u0627\u0633\u062A. \u0627\u06CC\u0646 \u0634\u0631\u06A9\u062A \u062F\u0631 \u0632\u0645\u06CC\u0646\u06CC \u0628\u0647 \u0645\u0633\u0627\u062D\u062A \u06F3\u06F5 \u06A9\u06CC\u0644\u0648\u0645\u062A\u0631 \u0645\u0631\u0628\u0639 \u062F\u0631 \u0646\u0632\u062F\u06CC\u06A9\u06CC \u0634\u0647\u0631 \u0645\u0628\u0627\u0631\u06A9\u0647 \u0648 \u062F\u0631 \u06F7\u06F5 \u06A9\u06CC\u0644\u0648\u0645\u062A\u0631\u06CC \u062C\u0646\u0648\u0628 \u063A\u0631\u0628\u06CC \u0634\u0647\u0631 \u0627\u0635\u0641\u0647\u0627\u0646 \u0648\u0627\u0642\u0639 \u0634\u062F\u0647\u200C\u0627\u0633\u062A. \u0645\u0635\u0631\u0641 \u0622\u0628 \u0627\u06CC\u0646 \u06A9\u0627\u0631\u062E\u0627\u0646\u0647 \u062F\u0631 \u06A9\u0645\u062A\u0631\u06CC\u0646 \u0645\u06CC\u0632\u0627\u0646 \u062E\u0648\u062F\u060C \u06F1\u066B\u06F5\u066A \u0627\u0632 \u062F\u0628\u06CC \u0632\u0627\u06CC\u0646\u062F\u0647\u200C\u0631\u0648\u062F \u0628\u0631\u0627\u0628\u0631 \u0633\u0627\u0644\u0627\u0646\u0647 \u06F2\u06F3 \u0645\u06CC\u0644\u06CC\u0648\u0646 \u0645\u062A\u0631 \u0645\u06A9\u0639\u0628 \u062F\u0631 \u0633\u0627\u0644 \u0627\u0633\u062A \u0648 \u062E\u0648\u062F \u06CC\u06A9\u06CC \u0627\u0632 \u0639\u0648\u0627\u0645\u0644 \u06A9\u0645\u200C\u0622\u0628\u06CC \u0632\u0627\u06CC\u0646\u062F\u0647\u200C\u0631\u0648\u062F \u0634\u0646\u0627\u062E\u062A\u0647 \u0645\u06CC\u200C\u0634\u0648\u062F." #@param {type:"string"}
question = "\u0645\u0635\u0631\u0641 \u0622\u0628 \u0641\u0648\u0644\u0627\u062F \u0645\u0628\u0627\u0631\u06A9\u0647 \u0686\u0642\u062F\u0631 \u0627\u0633\u062A\u061F" #@param {type:"string"}

# normalize input
context  = normalizer.normalize(context)
question = normalizer.normalize(question)

preds = predictor([question], [context], batch_size=1)
print(preds)
if preds[0]['text'].strip() == '':
  print('\n\n Model Prediction: Not Found!')
else:
  print('\n\n Model Prediction: ', preds[0]['text'].strip())

"""#**3-Print Model Structure**"""

print(model)

print(tokenizer)